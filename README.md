# Awesome Multimodal AI [![Awesome Lists](https://srv-cdn.himpfen.io/badges/awesome-lists/awesomelists-flat.svg)](https://github.com/awesomelistsio/awesome)

[![Ko-Fi](https://srv-cdn.himpfen.io/badges/kofi/kofi-flat.svg)](https://ko-fi.com/awesomelists) &nbsp; [![PayPal](https://srv-cdn.himpfen.io/badges/paypal/paypal-flat.svg)](https://www.paypal.com/donate/?hosted_button_id=3LLKRXJU44EJJ) &nbsp; [![Stripe](https://srv-cdn.himpfen.io/badges/stripe/stripe-flat.svg)](https://tinyurl.com/e8ymxdw3) &nbsp; [![X](https://srv-cdn.himpfen.io/badges/twitter/twitter-flat.svg)](https://x.com/ListsAwesome) &nbsp; [![Facebook](https://srv-cdn.himpfen.io/badges/facebook-pages/facebook-pages-flat.svg)](https://www.facebook.com/awesomelists)

> A curated list of models, tools, libraries, datasets, and resources for multimodal AI — systems that process and reason across multiple modalities such as text, image, audio, video, and sensor data.

## Contents

- [Models](#models)
- [Frameworks](#frameworks)
- [Image & Vision](#image--vision)
- [Audio & Speech](#audio--speech)
- [Video](#video)
- [Multimodal Agents](#multimodal-agents)
- [Datasets](#datasets)
- [Learning Resources](#learning-resources)
- [Related Awesome Lists](#related-awesome-lists)

## Models

- [OpenAI GPT-4o](https://openai.com/) – Multimodal model supporting text, vision, audio, and live interaction.
- [LLaVA](https://github.com/haotian-liu/LLaVA) – Large Language and Vision Assistant for image-grounded reasoning.
- [OmniModal (DeepMind)](https://deepmind.google/) – Multimodal models covering text, images, audio, and video.
- [Gemini](https://ai.google.dev) – Multimodal model capable of reasoning over text, images, and video.
- [Florence-2](https://github.com/microsoft/Florence-2) – Microsoft’s vision-language foundation model.
- [BLIP-2](https://github.com/salesforce/LAVIS) – Bootstrapped vision–language alignment for image reasoning.
- [Kosmos-2](https://github.com/microsoft/unilm) – Multimodal grounding and document parsing.
- [CLIP](https://github.com/openai/CLIP) – Contrastive vision–language model enabling multimodal embeddings.
- [ALIGN](https://github.com/google-research/vision_transformer) – Large-scale VLM for image–text alignment.

## Frameworks

- [LAVIS](https://github.com/salesforce/LAVIS) – Library for building and evaluating multimodal models.
- [HuggingFace Transformers](https://github.com/huggingface/transformers) – Supports a wide variety of VLMs, audio models, and multimodal architectures.
- [TorchMultimodal](https://github.com/facebookresearch/multimodal) – Framework for multimodal research from Meta.
- [OpenCLIP](https://github.com/mlfoundations/open_clip) – Reimplementation and extension of CLIP models.
- [MMDetection](https://github.com/open-mmlab/mmdetection) – OpenMMLab vision detection framework.
- [MMF (Modular Multimodal Framework)](https://github.com/facebookresearch/mmf) – Framework for vision + language tasks.

## Image & Vision

- [Stable Diffusion](https://github.com/Stability-AI/stablediffusion) – Diffusion-based generative image model.
- [ControlNet](https://github.com/lllyasviel/ControlNet) – Conditioning framework for image generation.
- [Segment Anything (SAM)](https://github.com/facebookresearch/segment-anything) – Foundation model for segmentation tasks.
- [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) – Pose detection for multimodal pipelines.
- [DINOv2](https://github.com/facebookresearch/dinov2) – Vision-only foundational embedding model.
- [LLaVA-Med](https://github.com/McMasterAI/LLaVA-Med) – Medical multimodal vision–language model.

## Audio & Speech

- [Whisper](https://github.com/openai/whisper) – Universal speech recognition and transcription.
- [NeMo ASR](https://github.com/NVIDIA/NeMo) – Framework for speech-to-text and multimodal models.
- [AudioCLIP](https://github.com/LAION-AI/audio-dataset-tools) – Audio–text–image representation learning.
- [SpeechT5](https://github.com/microsoft/SpeechT5) – Unified model for speech and text.
- [Meta MMS](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) – Massively multilingual speech models.

## Video

- [OpenAI Sora](https://openai.com/sora) – High-fidelity generative video model.
- [Stable Video Diffusion](https://github.com/Stability-AI/stable-video-diffusion) – Diffusion-based video generation.
- [Runway Gen-2](https://runwayml.com/) – Commercial multimodal video generation engine.
- [VideoCLIP](https://github.com/facebookresearch/fairseq/tree/main/examples/video) – Video–text embedding framework.
- [V-JEPA](https://github.com/facebookresearch/jepa) – Video joint embedding predictive architecture.

## Multimodal Agents

- [GPT-4o Live Agents](https://openai.com/) – Agents with real-time vision, speech, and reasoning.
- [LLaVA Agents](https://github.com/haotian-liu/LLaVA) – Vision-grounded agents for multimodal tasks.
- [HuggingFace Agents](https://github.com/huggingface/transformers/tree/main/src/transformers/agents) – Agents using tools with multimodal inputs.
- [MM-REACT](https://github.com/microsoft/visual-chatgpt) – Vision–language agent with tool-augmented reasoning.
- [Flamingo Agents](https://github.com/deepmind/flamingo) – Multimodal few-shot reasoning system.

## Datasets

- [LAION-5B](https://laion.ai/blog/laion-5b/) – Massive image–text dataset for VLM training.
- [COCO](https://cocodataset.org) – Image–caption dataset widely used in multimodal research.
- [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) – Large-scale image–text pairs.
- [HowTo100M](https://www.di.ens.fr/willow/research/howto100m/) – Video–text dataset for multimodal learning.
- [VQAv2](https://visualqa.org/) – Visual question answering dataset.
- [AudioSet](https://research.google.com/audioset/) – Large-scale audio event dataset.
- [AVSpeech](https://looking-to-listen.github.io/avspeech/) – Audio–visual speech dataset.

## Learning Resources

- [Multimodal Deep Learning Course](https://github.com/Atcold/pytorch-Deep-Learning) – Lectures covering multimodal architectures.
- [HuggingFace Multimodal Guides](https://huggingface.co/docs/transformers/index) – Tutorials for VLMs, audio models, and diffusion.
- [DeepMind Flamingo Paper](https://arxiv.org/abs/2204.14198) – Multimodal few-shot learning research.
- [BLIP-2 Paper](https://arxiv.org/abs/2301.12597) – Bootstrapping language–vision alignment.
- [CLIP Paper](https://arxiv.org/abs/2103.00020) – Landmark vision–language model.
- [LLaVA Paper](https://arxiv.org/abs/2304.08485) – Vision–language alignment with LLaMA.

## Related Awesome Lists

- [Awesome AI](https://github.com/awesomelistsio/awesome-ai)
- [Awesome Computer Vision](https://github.com/awesomelistsio/awesome-computer-vision)
- [Awesome Speech Processing](https://github.com/awesomelistsio/awesome-speech-processing)
- [Awesome AI Agents](https://github.com/awesomelistsio/awesome-ai-agents)
- [Awesome AI Research Tools](https://github.com/awesomelistsio/awesome-ai-research-tools)

## Contribute

Contributions are welcome!

## License

[![CC0](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by-sa.svg)](http://creativecommons.org/licenses/by-sa/4.0/)
